# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ahcEpnPLdvN8Gwpq3aCpeT6fv22zQQMa
"""

!pip install datasets transformers umap-learn hdbscan shap nltk scikit-learn --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import umap
import hdbscan
import shap
import nltk
nltk.download('punkt')

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.manifold import TSNE
from wordcloud import WordCloud

dataset = load_dataset("raeidsaqur/NIFTY", name="nifty-lm", split="train")
df = dataset.to_pandas()
df = df[['news']].dropna().drop_duplicates().reset_index(drop=True)
df['news'] = df['news'].astype(str)
df.head()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_name = "ProsusAI/finbert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)

def embed_text(texts, batch_size=16):
    embeddings = []
    model.eval()
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            tokens = tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(device)
            outputs = model(**tokens)
            emb = outputs.last_hidden_state[:, 0, :]
            embeddings.append(emb.cpu())
    return torch.cat(embeddings).numpy()

df = df.head(2000)
embeddings = embed_text(df['news'].tolist())

umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
umap_embeddings = umap_model.fit_transform(embeddings)

clusterer = hdbscan.HDBSCAN(min_cluster_size=20, metric='euclidean')
cluster_labels = clusterer.fit_predict(umap_embeddings)
df['cluster'] = cluster_labels

plt.figure(figsize=(10, 6))
sns.scatterplot(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], hue=df['cluster'], palette='tab10', s=20, legend='full')
plt.title('UMAP Projection of News Articles with HDBSCAN Clusters')
plt.show()

def extract_keywords(docs, n=10):
    tfidf = TfidfVectorizer(stop_words='english', max_features=1000)
    X = tfidf.fit_transform(docs)
    indices = np.argsort(tfidf.idf_)
    features = tfidf.get_feature_names_out()
    top_n = [features[i] for i in indices[:n]]
    return top_n

cluster_topics = {}
for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:
        continue
    texts = df[df['cluster'] == cluster_id]['news'].tolist()
    keywords = extract_keywords(texts)
    cluster_topics[cluster_id] = keywords
    print(f"Cluster {cluster_id}: {keywords}")

from collections import Counter

def cluster_keyword_purity(docs, top_n=10):
    tfidf = TfidfVectorizer(stop_words='english', max_features=1000)
    X = tfidf.fit_transform(docs)
    feature_names = tfidf.get_feature_names_out()
    summed = np.asarray(X.sum(axis=0)).flatten()
    keywords = [(feature_names[i], summed[i]) for i in np.argsort(summed)[::-1][:top_n]]
    return keywords

for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:
        continue
    docs = df[df['cluster'] == cluster_id]['news']
    top_keywords = cluster_keyword_purity(docs.tolist())
    print(f"\nCluster {cluster_id} Keyword Concentration:")
    for kw, score in top_keywords:
        print(f"{kw}: {score:.2f}")

for cluster_id, keywords in cluster_topics.items():
    print(f"\nCluster {cluster_id} - Keywords: {keywords}")
    sample_texts = df[df['cluster'] == cluster_id]['news'].sample(3, random_state=42)
    for i, txt in enumerate(sample_texts):
        print(f"\nSample {i+1}:\n{txt}")

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score

# Assumes the following variables from your existing pipeline:
# umap_embeddings, cluster_labels, documents

# --- FIGURE 1: UMAP Projection of FinBERT Embeddings ---
plt.figure(figsize=(10, 6))
plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=cluster_labels, cmap='Spectral', s=10)
plt.title("Fig. 1 - UMAP Projection of FinBERT Embeddings")
plt.xlabel("UMAP Dimension 1")
plt.ylabel("UMAP Dimension 2")
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# --- FIGURE 2: Cluster Size Distribution ---
plt.figure(figsize=(8, 5))
sns.countplot(x=cluster_labels)
plt.title("Fig. 2 - Cluster Size Distribution")
plt.xlabel("Cluster")
plt.ylabel("Number of News Items")
plt.grid(axis='y')
plt.show()

# --- FIGURE 3: TF-IDF Top Keywords Bar Chart per Cluster ---
# Example top keywords you previously extracted
tfidf_top_keywords = {
    0: [('stocks', 0.21), ('oil', 0.18), ('china', 0.15), ('new', 0.12), ('shares', 0.11)],
    1: [('oil', 0.22), ('stocks', 0.19), ('new', 0.17), ('china', 0.14), ('bank', 0.10)]
}

plt.figure(figsize=(10, 6))
for i, (cluster, keywords) in enumerate(tfidf_top_keywords.items()):
    terms, scores = zip(*keywords)
    plt.barh([f"{t} (C{cluster})" for t in terms], scores, label=f"Cluster {cluster}")
plt.xlabel("TF-IDF Score")
plt.title("Fig. 3 - Top Keywords by TF-IDF in Each Cluster")
plt.legend()
plt.tight_layout()
plt.show()

# --- FIGURE 4: Silhouette Score for Clustering ---
sil_score = silhouette_score(umap_embeddings, cluster_labels)
plt.figure(figsize=(5, 4))
plt.bar(["Silhouette Score"], [sil_score], color='skyblue')
plt.title("Fig. 4 - Silhouette Score of HDBSCAN Clustering")
plt.ylim(0, 1)
plt.text(0, sil_score + 0.02, f"{sil_score:.2f}", ha='center')
plt.show()

# --- FIGURE 5: Boxplot of Document Lengths per Cluster ---
doc_lengths = [len(doc.split()) for doc in df['news']]
df_lengths = pd.DataFrame({'cluster': cluster_labels, 'length': doc_lengths})

plt.figure(figsize=(10, 6))
sns.boxplot(x='cluster', y='length', data=df_lengths)
plt.title("Fig. 5 - Document Lengths per Cluster")
plt.xlabel("Cluster")
plt.ylabel("Word Count")
plt.grid(True)
plt.show()